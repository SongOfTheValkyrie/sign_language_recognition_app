{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_small\n",
    "from torchvision.models.mobilenetv3 import MobileNet_V3_Small_Weights\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gris/gris-f/homestud/jplapper/anaconda3/envs/signenv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/gris/gris-f/homestud/jplapper/anaconda3/envs/signenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "mobilenet_small = mobilenet_v3_small(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "mobilenet = mobilenet_small.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define the dataset path and set the batch size\n",
    "dataset_path = '/gris/gris-f/homestud/jplapper/SignLanguageApp/SigNN Character Database'\n",
    "batch_size = 64  # You can adjust this based on your system's memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=data_transforms['train'])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Now apply the appropriate transformations to each dataset split\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "val_dataset.dataset.transform = data_transforms['val']\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
    "for param in mobilenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Adjust the final layer according to the number of classes in your dataset\n",
    "num_ftrs = mobilenet.classifier[3].in_features\n",
    "NUM_CLASSES = 24\n",
    "mobilenet.classifier[3] = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilenet = mobilenet.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training loss: 1.9616945374686763\n",
      "Epoch 1/10, Validation accuracy: 65.89698046181172%\n",
      "Epoch 2/10, Training loss: 1.0443335057429548\n",
      "Epoch 2/10, Validation accuracy: 76.55417406749557%\n",
      "Epoch 3/10, Training loss: 0.7579102966020692\n",
      "Epoch 3/10, Validation accuracy: 84.84310242747188%\n",
      "Epoch 4/10, Training loss: 0.6023597419261932\n",
      "Epoch 4/10, Validation accuracy: 88.39550029603316%\n",
      "Epoch 5/10, Training loss: 0.5032054753798358\n",
      "Epoch 5/10, Validation accuracy: 89.40201302545886%\n",
      "Epoch 6/10, Training loss: 0.43164440595878745\n",
      "Epoch 6/10, Validation accuracy: 90.05328596802842%\n",
      "Epoch 7/10, Training loss: 0.3787866455485236\n",
      "Epoch 7/10, Validation accuracy: 91.53345174659562%\n",
      "Epoch 8/10, Training loss: 0.33506086075081015\n",
      "Epoch 8/10, Validation accuracy: 92.1255180580225%\n",
      "Epoch 9/10, Training loss: 0.3084867177706844\n",
      "Epoch 9/10, Validation accuracy: 91.88869153345175%\n",
      "Epoch 10/10, Training loss: 0.280599260667585\n",
      "Epoch 10/10, Validation accuracy: 92.36234458259325%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.classifier[3].parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mobilenet.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Training loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation accuracy\n",
    "    mobilenet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = mobilenet(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Validation accuracy: {100.0 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mobilenet.state_dict(), 'asl_recognition_mobilenet.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gris/gris-f/homestud/jplapper/anaconda3/envs/signenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /gris/gris-f/homestud/jplapper/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100%|██████████| 171M/171M [00:02<00:00, 88.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "resnet101 = models.resnet101(pretrained=True)\n",
    "\n",
    "# Freeze the model parameters\n",
    "for param in resnet101.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the final layer according to the number of classes in your dataset\n",
    "num_ftrs_resnet = resnet101.fc.in_features\n",
    "NUM_CLASSES = 24\n",
    "resnet101.fc = nn.Linear(num_ftrs_resnet, NUM_CLASSES)\n",
    "\n",
    "# Transfer the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet101 = resnet101.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training loss: 2.0801115542087913\n",
      "Epoch 1/10, Validation accuracy: 64.89046773238603%\n",
      "Epoch 2/10, Training loss: 1.1397516018939469\n",
      "Epoch 2/10, Validation accuracy: 80.69863824748371%\n",
      "Epoch 3/10, Training loss: 0.8339551124932632\n",
      "Epoch 3/10, Validation accuracy: 83.30373001776199%\n",
      "Epoch 4/10, Training loss: 0.6618601215897866\n",
      "Epoch 4/10, Validation accuracy: 85.7312018946122%\n",
      "Epoch 5/10, Training loss: 0.5606912377308\n",
      "Epoch 5/10, Validation accuracy: 87.56660746003553%\n",
      "Epoch 6/10, Training loss: 0.4821411219407927\n",
      "Epoch 6/10, Validation accuracy: 88.86915334517467%\n",
      "Epoch 7/10, Training loss: 0.43507545005600406\n",
      "Epoch 7/10, Validation accuracy: 88.57312018946122%\n",
      "Epoch 8/10, Training loss: 0.377035675605513\n",
      "Epoch 8/10, Validation accuracy: 89.81645944345767%\n",
      "Epoch 9/10, Training loss: 0.35328672348328355\n",
      "Epoch 9/10, Validation accuracy: 90.34931912374186%\n",
      "Epoch 10/10, Training loss: 0.32240408082615657\n",
      "Epoch 10/10, Validation accuracy: 91.71107164002369%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_resnet = optim.Adam(resnet101.fc.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    resnet101.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer_resnet.zero_grad()\n",
    "        outputs = resnet101(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_resnet.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Training loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation accuracy\n",
    "    resnet101.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = resnet101(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Validation accuracy: {100.0 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet101.state_dict(), 'asl_recognition_resnet101.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
